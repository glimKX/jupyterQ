{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WHAT QUESTION DO I WANT TO ANSWER??__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning One Hot Encoding in Q\n",
    "First we take a peek into the date and decide on the datatype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\\c 200 200\n",
    "headers:\",\" vs raze 1#read0 `:Datasets/vgsales.csv\n",
    "data:\",\" vs raze 1_2#read0 `:Datasets/vgsales.csv\n",
    "headers!data\n",
    "/rank is id - J\n",
    "/Name is unique - C\n",
    "/Platform is repeated string - S\n",
    "/Year is year - D\n",
    "/Genre is repeated string - S\n",
    "/Publisher is repeated string -S\n",
    "/NA_Sales - F\n",
    "/EU_Sales - F\n",
    "/JP_Sales - F\n",
    "/Other_Sales - F\n",
    "/Global_Sales - F\n",
    "\n",
    "10#vgsales:(\"J*S*SSFFFFF\";enlist \",\") 0: `:Datasets/vgsales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumeration as Categorical Encoding\n",
    "In kdb q, we are able to enumerate the symbols and this allows us to jump between int and sym category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`Wii`NES\r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "`Sports`Platform\r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "`Nintendo`Microsoft Game Studios\r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Rank Name                       Platform Year   Genre        Publisher NA_Sales EU_Sales JP_Sales Other_Sales Global_Sales\r\n",
       "--------------------------------------------------------------------------------------------------------------------------\r\n",
       "1    \"Wii Sports\"               Wii      \"2006\" Sports       Nintendo  41.49    29.02    3.77     8.46        82.74       \r\n",
       "2    \"Super Mario Bros.\"        NES      \"1985\" Platform     Nintendo  29.08    3.58     6.81     0.77        40.24       \r\n",
       "3    \"Mario Kart Wii\"           Wii      \"2008\" Racing       Nintendo  15.85    12.88    3.79     3.31        35.82       \r\n",
       "4    \"Wii Sports Resort\"        Wii      \"2009\" Sports       Nintendo  15.75    11.01    3.28     2.96        33          \r\n",
       "5    \"Pokemon Red/Pokemon Blue\" GB       \"1996\" Role-Playing Nintendo  11.27    8.89     10.22    1           31.37       \r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Rank Name                       Platform Year   Genre Publisher NA_Sales EU_Sales JP_Sales Other_Sales Global_Sales\r\n",
       "-------------------------------------------------------------------------------------------------------------------\r\n",
       "1    \"Wii Sports\"               0        \"2006\" 0     0         41.49    29.02    3.77     8.46        82.74       \r\n",
       "2    \"Super Mario Bros.\"        1        \"1985\" 1     0         29.08    3.58     6.81     0.77        40.24       \r\n",
       "3    \"Mario Kart Wii\"           0        \"2008\" 2     0         15.85    12.88    3.79     3.31        35.82       \r\n",
       "4    \"Wii Sports Resort\"        0        \"2009\" 0     0         15.75    11.01    3.28     2.96        33          \r\n",
       "5    \"Pokemon Red/Pokemon Blue\" 2        \"1996\" 3     0         11.27    8.89     10.22    1           31.37       \r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/Here we will enumerate platform, genre, and publisher\n",
    "2#distPlatform:exec distinct Platform from vgsales\n",
    "2#distGenre:exec distinct Genre from vgsales\n",
    "2#distPublisher: exec distinct Publisher from vgsales\n",
    "5#vgEumsales:update `distPlatform $ Platform, `distGenre$Genre, `distPublisher$Publisher from vgsales\n",
    "\n",
    "/\n",
    "The above will enumerate the sym (category) into specific placeholder such as distPlatform\n",
    "This means that we are able to use \"i\"${item} to cast it back to category number\n",
    "Here we try to change them into int before creating the one hot encoding columns\n",
    "\\\n",
    "5#vgTmpSale:update \"i\"$Platform,\"i\"$Genre,\"i\"$Publisher from vgEumsales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Encoding with One Hot\n",
    "From the enumerated table, we seek to create additional columns that represents each category in kdb Q\n",
    "To do so, we make use of dictionary which is mroe flexible than tables, but this can be done with q-sql syntax as well\n",
    "<br> Here we use the flexibility of q to synthetically produce new columns which contains the one hot encoding\n",
    "<br> This will not be the most efficient way if there are too many categories, if thats the case, we might want to use hash featuring\n",
    "> Hash featuring, might be considered in the future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newPlatformColumns| 31\r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "newGenreColumns| 12\r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "newPublisherColumns| 579\r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Publisher_574 Publisher_575 Publisher_576 Publisher_577 Publisher_578\r\n",
       "---------------------------------------------------------------------\r\n",
       "0             0             0             0             0            \r\n",
       "0             0             0             0             0            \r\n",
       "0             0             0             0             0            \r\n",
       "0             0             0             0             0            \r\n",
       "0             0             0             0             0            \r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "`Rank`Name`Platform`Year`Genre`Publisher`NA_Sales`EU_Sales`JP_Sales`Other_Sales`Global_Sales`Platform_0`Platform_1`Platform_2`Platform_3`Platform_4`Platform_5`Platform_6`Platform_7`Platform_8`Platf..\r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "633\r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/Here we use the flexibility of q to synthetically produce new columns which contains the one hot encoding\n",
    "/This will not be the most efficient way if there are too many categories, if thats the case, we might want to use hash featuring\n",
    "/Hash featuring, might be considered in the future especially for Publisher which has 579\n",
    "\n",
    ".encode.createNewColumns:{[enumtable;column] `$string[column],/:\"_\" ,/: string each distinct enumtable[column]}\n",
    ".encode.createNewData:{[enumtable;column] enumtable[column] =/: distinct enumtable[column]}\n",
    "\n",
    "enlist[`newPlatformColumns]!enlist count newPlatformColumns:.encode.createNewColumns[vgTmpSale;`Platform]\n",
    "newPlatformData:.encode.createNewData[vgTmpSale;`Platform]\n",
    "\n",
    "enlist[`newGenreColumns]!enlist count newGenreColumns:.encode.createNewColumns[vgTmpSale;`Genre]\n",
    "newGenreData:.encode.createNewData[vgTmpSale;`Genre]\n",
    "\n",
    "enlist[`newPublisherColumns]!enlist count newPublisherColumns:.encode.createNewColumns[vgTmpSale;`Publisher]\n",
    "newPublisherData:.encode.createNewData[vgTmpSale;`Publisher]\n",
    "\n",
    "5#-5#'vgSaleFinal:vgTmpSale,'flip[newPlatformColumns!newPlatformData],'flip[newGenreColumns!newGenreData],'flip[newPublisherColumns!newPublisherData]\n",
    "\n",
    "/A look at the final table columns\n",
    "cols vgSaleFinal\n",
    "count cols vgSaleFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Featuring\n",
    "With Hash Featuring we can compress hundred of columns into a small bunch, minimizing computational needs\n",
    "<br> This utilises sklearn python library which makes this complicated problem into a simple liner\n",
    "<br> If we use this function, we do not have to enumerate the data and this could be done from the on start. \n",
    "<br> Both has its pros and cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fhPublisher_0| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 ..\r\n",
       "fhPublisher_1| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  -3 -3 0  0  0  0  0  -3 -3 0  0  0  0  0  0  0  0  0  0  0  0  0  -3 0  0  0  0  0  -3 0  0  0  0  0  0  -3 0  0  0  0  -3 0  0  0  0 ..\r\n",
       "fhPublisher_2| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  1  1  0  0  0  0  0  1  1  0  0  0  2  0  0  0  0  0  0  0  0  0  1  0  0  0  0  2  1  0  0  2  0  0  0  1  2  0  2  0  1  0  0  0  0 ..\r\n",
       "fhPublisher_3| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 ..\r\n",
       "fhPublisher_4| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  1  0  0  0  0  0  0 ..\r\n",
       "fhPublisher_5| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 ..\r\n",
       "fhPublisher_6| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  -1 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  -1 0  0  0  0  -1 0  -1 0  0  0  0  0  0 ..\r\n",
       "fhPublisher_7| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 ..\r\n",
       "fhPublisher_8| 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 ..\r\n",
       "fhPublisher_9| -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -3 -2 -2 -1 -1 -1 -1 -1 -2 -2 -1 -1 -1 -4 -2 -1 -2 -1 -2 -2 -2 -2 -2 -2 -1 -2 -1 -1 -3 -2 -1 -1 -4 -1 -1 -1 -2 -4 -1 -4 -2 -2 -1 -1 -1 -2..\r\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureHashing:{[nFeature;inputType;data;colName]\n",
    "    FeatureHasher:.p.import[`sklearn.feature_extraction][`:FeatureHasher];\n",
    "    fh:FeatureHasher[`n_features pykw nFeature;`input_type pykw inputType];\n",
    "    hashed_features:fh[`:fit_transform] data;\n",
    "    hashed_features:hashed_features[`:toarray][];\n",
    "    data:hashed_features`;\n",
    "    colName:`$,[\"fh\";colName],/:\"_\",/: string til count first `float$data;\n",
    "    flip colName!flip data\n",
    "    }\n",
    "\n",
    "10#flip featureHashing[30;\"string\";vgsales `Publisher;\"Publisher\"]\n",
    "\n",
    "/\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "fh = FeatureHasher(n_features=6, input_type='string')\n",
    "hashed_features = fh.fit_transform(vg_df['Genre'])\n",
    "hashed_features = hashed_features.toarray()\n",
    "pd.concat([vg_df[['Name', 'Genre']], pd.DataFrame(hashed_features)], \n",
    "          axis=1).iloc[1:7]\n",
    "\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Sales from Genre and Publisher\n",
    "Is there predictive power from genre or publisher for potential sales generated ?\n",
    "- Correlation of Genre against sales\n",
    "- Correlation of publisher against sales\n",
    "- Is there a need for cross encoding ?\n",
    "- Is there a need to keep both genre and publisher (correlation test, high correlation means not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Test\n",
    "By correlating datasets, we are able to tell if we can remove overlapping data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre     | 0 1 2 0 3 4 1 5 1 6 7 2 3 0 0 5 8 8 1 5 3 1 1 8 8 3 3 4 2 6 3 6 3 6 6 6 6 6 8 9 6 7 2 6 8 8 1 2 1 3 10 8 2 1 2 6 8 1 3 1 5 6 6 2 1 6 3 6 5 2 6 1 5 7 5 3 2 0 5 6 5 3 8 7 6 0 3 3 3 4 8 1 ..\r\n",
       "Platform_0| 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 ..\r\n",
       "Platform_1| 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ..\r\n",
       "Platform_2| 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ..\r\n",
       "Platform_3| 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 ..\r\n",
       "Platform_4| 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0  1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ..\r\n",
       "Platform_5| 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0  0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ..\r\n",
       "Platform_6| 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ..\r\n",
       "Platform_7| 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ..\r\n",
       "Platform_8| 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ..\r\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlationColumns:`Genre,newPlatformColumns\n",
    "correlationColumns:?[vgSaleFinal;();0b;correlationColumns!correlationColumns]\n",
    "\n",
    "10#flip correlationColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a         b         c         \r\n",
      "------------------------------\r\n",
      "0.726781  0.6708738 0.7520102 \r\n",
      "0.4046546 0.6789082 0.1086824 \r\n",
      "0.8355065 0.412317  0.9598964 \r\n",
      "0.642737  0.9877844 0.03668341\r\n",
      "0.5830262 0.3867353 0.6430982 \r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " | a          b          c         \r\n",
       "-| --------------------------------\r\n",
       "a| 1          -0.2262263 0.739021  \r\n",
       "b| -0.2262263 1          -0.7780611\r\n",
       "c| 0.739021   -0.7780611 1         \r\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/Correlation matrix\n",
    "show t:([]a:5?1.0;b:5?1.0;c:5?1.0)\n",
    "u cor/:\\:u:flip t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Sales Publisher vs Big Publisher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\\dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q 3.5",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
